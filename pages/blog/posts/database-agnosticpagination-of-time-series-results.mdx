import { BlogPostLayout } from "@/components/layout";

export const meta = {
    title: "Database Agnostic Pagination of Time-Series Results",
    publishedDate: "2022-06-02",
    description:
        "A way to paginate time series results in DB side even when the DB itself doesn't support it.",
    mainImage: {
        src: "/assets/blog/database-agnosticpagination-of-time-series-results/main.jpeg",
        alt: "Database Agnostic Pagination of Time-Series Results - Main Image",
        blurDataURL:
            "data:image/webp;base64,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",
        source: {
            author: "Aron Visuals",
            authorUrl:
                "https://unsplash.com/@aronvisuals?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText",
            source: "Unsplash",
            sourceUrl:
                "https://unsplash.com/s/photos/time-series?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText",
        },
    },
};

If you are working with Observability or any Data Science field, chances are that you have to work with time-series
data at one point or the other. This is because most practical applications of data science will include a time
dimension and it is generally useful to preserve this information in many cases.

A common issue when dealing with such time-series data, is the sheer volume of data that is collected and the
challenges that comes along with dealing with such data.

Today in this article, I will be focusing on one such issue, which is paginating time series data. Pagination is
important in any solution and helps clients to process or visualize data without getting overwhelmed by the the sheer
volume of all the relevant data.

Moreover, most large scale time-series databases has hard limits on the number of results that can be returned in a
single query (For Azure Data Explorer this was set at 500,000 rows or 64 MB (whichever is lower) at the time I was
writing the article). This can result in problems if you wish to query large amounts of data through the REST APIs.

In this article, I will be walking you through a solution which we had to implement due to the lack of database side
pagination support in Azure Data Explorer and in other products powered by it. However, the approach used in this
article is **database agnostic** and it should work with any time-series database out there.

---

Now let’s take a look how to implement pagination in time-series data even without any support for it from the
time-series database itself.

**Note:** I have assumed that the time series database will return an error when database limits are hit along with a
partial results set. This is simply to follow the behavior of the Azure Data Explorer that I tested this with. However,
if only an error is returned without a partial results set, the algorithm can still be modified to address it).

![Idea](//assets/blog/database-agnosticpagination-of-time-series-results/idea.jpeg)

# Pseudo Code for Chunking of Time-series Query Results

**Input:**

-   _start_time_ - The last timestamp (inclusive) to include in results
-   _end_time_ - The last timestamp (exclusive) to include in results
-   _limit_ - The maximum number of rows to fetch in a single query

**Utility Functions:**

-   _query_database_ - Query the database and return results in ascending order
-   _process_ - Process an array of results
-   _size_of_ - Get length of an array
-   _timestamp_of_ - Get the timestamp of a row of results
-   _is_server_side_limit_error_ - Returns true if the error passed in is the server side limit error

**Algorithm:**

```python
current_start_time = start_time
while True:
    has_reached_server_side_limit = False
    rows = []
    try:
        rows = query_database(current_start_time, end_time, limit)
    except Exception as err:
        if is_server_side_limit_error(err):
            has_reached_server_limit = True
            rows = err.partial_results.rows  # Partial results set
        else:
            raise err
    row_count = size_of(rows)
    if row_count == limit or has_reached_server_side_limit:
        last_timestamp = timestamp_of(rows[row_count - 1])
        i = size_of(rows)
        while i > 0 && timestamp_of(rows[i-1]) == last_timestamp:
            i--
        if i == 0:
            raise Error("Unsupported condition: limit too small")
        rows = rows[0:i] # Dropping all rows with last timestamp
        current_start_time = last_timestamp
        process(rows)
    else:
        if row_count > 0:
            process(rows)
        break
```

# Explanation

The most important aspect of any pagination algorithm is the way to continue from the last results chunk. In this
algorithm, this is achieved by querying the results in the ascending order and using the last timestamp of the previous
results chunk as the start timestamp of the next results chunk.

However, to avoid any overlaps due to repeated timestamps in multiple rows, we need to always drop the last timestamp
in the chunk so that we can fetch results starting from that point to the end of the time range (Line 14–21). There is
an inherent limitation because of this approach, which is explained below.

The terminating condition of the querying is detected when the number of returned data is less than the limit specified
and if there were no database side limit errors (Line 24–27).

# Limitations & ways to overcome them

While this algorithm will work for many cases, if your data contains multiple repeated timestamps in rows, the
algorithm may fail. This can happen if the limit applied to results chunk is smaller than the maximum number of
repeated timestamps as we are always removing the last. The algorithm will raise an error if this condition is reached
(Line 19–20).

However, in many real world applications repeating timestamps in multiple events will be rare. However, there can be
some cases where there are repeated timestamps due to the implementation of how these data points were recorded.
(e.g.:- multiple sensor readings which has the same timestamp, to simplify the sensor reading implementation, even
though they were actually read at least several milliseconds apart).

One approach to overcome these limitations in such scenarios, would be to group these events into a single row using
the time-series database query itself.

---

# Conclusion

As you have witnessed above, this approach would work with almost any database without any issues.

However, I would like to emphasize the fact that, **we should not try to query all data in this manner unless it is
absolutely necessary**. Querying data in this manner will put a lot of strain on the database as well as the client.

You should always try to **aggregate and process the data within the database itself using database queries**. Most of
the time-series databases available today includes rich enough syntax to perform such processing at database side
itself.

However, in those rare occasions when you have no other alternative (e.g.:- if you are storing logs in an Azure Data
Explorer and would like to implement a logs viewer), hope this approach would save your day.

export default ({ children }) => (
    <BlogPostLayout metadata={meta}>{children}</BlogPostLayout>
);
